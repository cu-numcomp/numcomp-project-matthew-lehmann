{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: **Newton's Method** in PetterS/spii Nonlinear Optimization of Multivariable Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the method\n",
    "\n",
    "<!-- How does the method relate to concepts we've covered in class. You're writing for your fellow classmates so try to make it understandable to them. Why is the method used in this context? Can you explain why it's preferred over some alternative in this context? -->\n",
    "\n",
    "This Git resource PetterS/spii is a method for the unconstrained minimization of smooth functions with many variables. The best way to try and understand what this means is to consider a simple example. Consider the function <br>\n",
    "<center>$ f(x,y) = x^2 y $ </center><br>\n",
    "and say that you want to minimze the function on the set where <br>\n",
    "<center> $ x^2 + y^2 = 1 $ </center> <br>\n",
    "If you envision the 2-D view of this function $ f(x) $ you can imagine plotting the set $ x^2 + y^2 = 1$ as a red circle that contains different values from $ f(x) $. This circle of points would be the constraint in this problem. And minimizing or maximizing the function within this constraint would be finding the minimum or maximum values of the function along or within (depending on the problem statement) the bounds of this circle.  \n",
    "<br>\n",
    "<br>\n",
    "Now, we know Newton's method as a process by which we can find the roots of a given function. But when it comes to optimization (as this GitHub resource does) Newton's method can be applied to the derivative of a twice differentiable function to find solutions to $ f'(x) = 0 $. These are of course points where the rate of change of the original function is $ 0 $. So essentially, the application of the Newton's method we learned in class, is being applied not to a function $f(x)$ but to its derivative to find more interesting behavior in the $f(x)$ function. To the best of my knowledge, this kind of an application of Newton's method is used in machine learning algorithms quite often. And it is applied to functions of multiple variables like the $f(x,y)$ we used as an example previously.\n",
    "<br>\n",
    "<br>\n",
    "Overall, the author uses Newton's method in his source code because he is using the code to better understand nonlinear optimization. Perhaps he is someone who deals with a lot of machine learning type problems, or he simply wanted to compare the effectiveness of Newton's method and the quasi family of Newton's method approximations that will be discussed in the next section. In general, Newton's method here is used to understand and find the extrema of very complex functions without any specific constraint like the one we mentioned above. So that regardless of the complexity of the multivariable function being used in the problem (perhaps some AI application) the entire function can be searched for extrema. \n",
    "\n",
    "## Alternative to Newton's Method in this Context\n",
    "What is known as L-BFGS (Limited Memory Broyden–Fletcher–Goldfarb–Shanno algorithm) is apparently another optimization algorithm that this Git repository code can implement in minimizing these smooth functions. It is a part of a family known as quasi-Newton methods that are known to approximate Newton's method by approxoimating the Hessian instead of actually directly solving for it. The Hessian is essentially the matrix of a functions second partial derivatives. This is similar to the Jacobian we are familiar with from calculus which includes the first partial derivatives of a function. Overall, it's important to note that Newton's method, when applied for minimizing smooth functions of many variables (our example only has 2) requires the Hessian for there NOT to be a constraint. The L-BFGS and other non-quasi newton's methods can approximate this Hessian rather than solve for it and thus save on some computer memory. However, the Newton's method is used in the context of this Git source due to its ability to handle nonconvex functions (functions with many local minima). The Hessian becomes key since it is used for finding the extrema in functions and when local extrema enter the mix, a Bunch-Kaufman-Parlett factorization and block-diagonal modification are applied to the PetterS/spii method to handle the nonconvex functions. Thus Newton's method is the first option over the L-BFGS in this author's code.\n",
    "\n",
    "### Questions I still have about the method\n",
    "\n",
    "* How effective is Newton's method in these multivariable functions extrema algorithms? Is it fully applicable?\n",
    "* What exactly does a multivariable function in machine learning look like as an example? I couldn't find one, but I can definitely see how a reward function that depends on multiple parameters a machine must consider can be analyzed with this software. You want to find the most valuable portion of this strange multivariable function to optimize the AI decision making.\n",
    "\n",
    "## About the software\n",
    "Link to repository: https://github.com/PetterS/spii\n",
    "<br>\n",
    "<br>\n",
    "The package is essentially a tool that the main contributor created to give himself a better understanding of nonlinear optimization problems. Now, what it can be applied for, according to research, appears to be mainly machine learning principles. Imagine how useful it might be, not to only analyze the cost functions of a robot learning to distinguish between objects, but how useful it would be to minimize that cost function. Or, considering most AI works on reward type functions (where the maximum amount of reward determines its decisions) how valuable it would be to find the extrema of these reward functions to monitor the performance of the AI. Moreover, this software finds the extrema of multivariable functions WITHOUT a given constraint. Meaning that the entirety of the function's domain can be searched in order to find any extrema.\n",
    "<br>\n",
    "<br>\n",
    "At a high level it appears to be developed to help with machine learning, which would make sense considering the fact that the creator Petter Strandmark works with computer vision. It appears to be mainly written in C++ and again, is mostly developed and used by machine learning researchers. \n",
    "<br>\n",
    "<br>\n",
    "Here is an image that helps visualize what extrema searching looks like on these kinds of functions (visualized with a contour map). \n",
    "<!-- Link to the repository. What does the software package do (at a high level)? Who develops it? Who uses it? What language is it written in and what language(s) can it be called from? If there are figures of its architecture, use, or products (e.g., from the docs), you're welcome to include them here. This is an example diagram included in the notebook: -->\n",
    "\n",
    "![](https://aria42.com/images/steepest-descent.png)\n",
    "<br>\n",
    "<br>\n",
    "The software can be compiled in Visual Studio 2015, GCC 4.9 (Cygwin), GCC 4.9 (Ubuntu), Clang 3.5 (Ubuntu) Earlier compilers may not work (according to the contributor). \n",
    "\n",
    "## Method as it appears in the software\n",
    "\n",
    "<!-- What role does the method play in the software? How does one call it (perhaps via a higher level interface that uses the method)? Are there particular performance concerns that went into its use? How expensive is it? Can you express performance in terms that are relevant to a user? How about accuracy, conditioning, or stability in the chosen formulation? -->\n",
    "\n",
    "As discussed in the first section, Newton's method is used in this software a little differently from how we used it in class. It is used to find the extrema of the second derivative of multivariable functions. You don't call Newton's method itself from the software (it's simply built-in and runs syncrhonously with the overall program) but the higher-level search for the extrema of your multivariable function will employ it. \n",
    "<br>\n",
    "<br>\n",
    "This was also discussed earlier, briefly, but there are some performance considerations taken into account. The software allows the user to utilize the Newton's method version of the program or the L-BFGS quasi Newton's method version in which the Hessian is approximated for improved CPU performance. However, this higher performing method does struggle more with nonconvex functions when more variables are introduced into the function being analyzed. The Newton's method version of this software has Bunch-Kaufman-Parlett factorization and block-diagonal modification to deal with these nonconvex functions (functions that have many local extrema). \n",
    "### Open questions\n",
    "\n",
    "* Im not certain what Bunch-Kaufman-Parlett factorization and block-diagonal modification really is after researching it. Is there any sort of explanation as to how exactly it handles functions with multiple local extrema?\n",
    "* Could I get a better explanation than I provided for the Hessian? Not necessarily about what it actually is, moreso about why it is actually necessary in this kind application of Newton's method. I'm guessing the answer is pretty mathematically involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
